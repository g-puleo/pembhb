batch_size: 50
epochs: 1000
learning_rate: 0.00001
#transform_td: "normalise_max" # options: "none", "logwhiten", "log", "whiten"
transform_fd: "log" # options: "none", "logwhiten", "log", "whiten", "normalise_max"
early_stop_patience: 50
scheduler_factor: 0.3
scheduler_patience: 10
  
marginals: 
  #  f: [[0]]
  f: [[0], [1], [0,1]]


device: "cuda"


architecture:
  gradnorm:
    enabled: True
    alpha: 1
    learning_rate: 0.0001
    common_params: "linear_f"


  data_summary:

    type: "PeregrineModel"
    BrutalCompression:
      Nfreqs: 100
      lowdim: 16
      in_channels: 4

    PeregrineModel:
      n_channels: 2 # number of channels used (AE--> 2, AET-->3)
      n_timesteps: 6048
      n_freqs: 4096
      # Nfreqs: 100
      # lowdim: 16
      # in_channels: 6
      # cnn_channels: [32, 64]


  
